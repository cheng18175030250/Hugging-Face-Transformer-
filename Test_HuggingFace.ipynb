{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b8521a-00d7-461d-bcb2-d1f40a280e8e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: green; padding: 10px\">\n",
    "    一. Transformers库的基本介绍及说明\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf5dbe-b5b2-4317-991c-9c9196dcda41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "    Transformers 库中最基本的对象是pipeline()函数。它将模型与其必要的预处理和后处理步骤连接起来，使我们能够直接输入任何文本并获得答案：\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808f6fba-d53d-48c2-aff9-19d0fdd05b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 10:42:36.455422: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598051905632019},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "classifier( [\n",
    "    \"I' ve been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e30bc-bbab-4386-afd1-e5f99346d092",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10\">\n",
    "目前可用的一些pipeline 有：\n",
    "1. feature-extraction 特征提取：把一段文字用一个向量来表示\n",
    "2. fill-mask 填词：把一段文字的某些部分mask住，然后让模型填空\n",
    "3. ner 命名实体识别：识别文字中出现的人名地名的命名实体\n",
    "4. question-answering 问答：给定一段文本以及针对它的一个问题，从文本中抽取答案\n",
    "5. sentiment-analysis 情感分析：一段文本是正面还是负面的情感倾向\n",
    "6. summarization 摘要：根据一段长文本中生成简短的摘要\n",
    "7. text-generation文本生成：给定一段文本，让模型补充后面的内容\n",
    "8. translation 翻译：把一种语言的文字翻译成另一种语言\n",
    "    zero-shot-classification\n",
    "ication\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f729f2-a85e-441b-9a06-8f12faedc33a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "1. Pipeline 背后的流程: \n",
    "\n",
    "在接收文本后，通常有三步：Tokenizer、Model、Post-Processing。\n",
    "\n",
    "1）Tokenizer\n",
    "\n",
    "与其他神经网络一样，Transformer 模型不能直接处理原始文本，故使用分词器进行预处理。使用AutoTokenizer类及其from_pretrained()方法。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124962a7-6ac9-4892-a944-d6cd7e068231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 2023, 2607, 2026, 2878,\n",
      "         2166, 1012,  102],\n",
      "        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" #根据这个模型所对应的来加载\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) \n",
    "\n",
    "raw_inputs = [\"I've been waiting for a this course my whole life.\", \"I hate this so much!\",]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360e2a85-fe39-4070-b7d8-53eeb9cb5cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] i've been waiting for a this course my whole life. [SEP]\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 2023, 2607, 2026, 2878,\n",
    "         2166, 1012,  102])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152af868-346d-419e-bee9-d6827d2e0df5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "2）Model\n",
    "    \n",
    "Transformers 提供了一个AutoModel类，它也有一个from_pretrained()方法：\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "254cede3-c326-4548-8c64-7c74f2d780d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc866ccd-b598-46d5-8332-4082efe00c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b26d8-36ad-4721-90fb-a9c9485676a9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "如果我们将预处理过的输入提供给我们的模型，我们可以看到：\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed0207cf-a11c-4a32-9f7b-9d2068e6e386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769d76b5-779c-4497-af69-2fb4db2d09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef71e3-f939-4ce0-b879-d8db2d41b7e3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "3）Post-Processing\n",
    "\n",
    "模型最后一层输出的原始非标准化分数。要转换为概率，它们需要经过一个SoftMax层，所有 Transformers 模型都输出 logits，因为用于训练的损耗函数一般会将最后的激活函数(如SoftMax)与实际损耗函数(如交叉熵)融合 。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19bd05b1-711c-4b52-a521-ab23343f962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5446e-02, 9.8455e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "prediction = torch.nn.functional.softmax(outputs.logits,dim = 1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895a4aa9-ade0-4cf9-84e2-718ca3671c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27f7f3-4797-471d-9f12-28aac2c66d7c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "2. Models\n",
    "\n",
    "1）创建Transformer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa575ad-79f4-4a7d-972c-3573caa32415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff467a6-5b55-4a2c-a5ad-e5cba00cf595",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "2）不同的加载方式\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27fcc7a-43aa-44f1-9798-7329e7c1de45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28732a81598c4140b000daa24e3f7b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec97c6b2040745f68873fc103cb43c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bf383-908b-468b-8ee5-6a675d281fad",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "3）保存模型\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb35bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebcc0a-d129-4d0a-afbe-b5b152a370bf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "4）使用Transformer model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4422c66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-3.7235,  3.9691],\n",
      "        [-4.2219,  4.5807],\n",
      "        [-4.2852,  4.6166]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n",
    "\n",
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)\n",
    "output = model(model_inputs)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c7ef3-84ef-4c89-8c55-edb6aab12125",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "3. Tokenizers\n",
    "    \n",
    "1）Loading and saving\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fbc733a-df8f-4ab3-af28-48df10258051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccb7eea884c4eec81d1bd8750982663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d6ff82e365488684f4b12ce39c6a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer(\"Using a Transformer network is simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b80660f0-3e2f-472a-8afb-377adbe384ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('directory_on_my_computer/tokenizer_config.json',\n",
       " 'directory_on_my_computer/special_tokens_map.json',\n",
       " 'directory_on_my_computer/vocab.txt',\n",
       " 'directory_on_my_computer/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37076426-1e38-4755-a4dd-fdfd8863b5e1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "2）Tokenization\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "322d492f-f193-42af-99b1-454d72e0812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
      "[7993, 170, 11303, 1200, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens) # 输出 : ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
    "\n",
    "#  从token 到输入 ID\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids) # 输出：[7993, 170, 11303, 1200, 2443, 1110, 3014]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb83592-accd-4b37-9074-6420b8922d4a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "3） Decoding\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47349b8-e7d3-4aa6-9dcd-ac28c034d06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string) # 输出：'Using a Transformer network is simple'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf68d68-aa5a-4be9-8f56-2b161129a28f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "4. 处理多个序列 Handling multiple sequences\n",
    "    \n",
    "（1） 模型需要一批输入 Models expect a batch of inputs\n",
    "\n",
    "将数字列表转换为张量并将其发送到模型：\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca021e1c-4217-4721-9242-e67c9b13142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nInput IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]\\nLogits: [[-2.7276,  2.8789]]\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)\n",
    "\n",
    "# 输出\n",
    "'''\n",
    "Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]\n",
    "Logits: [[-2.7276,  2.8789]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9a0c8-0113-4d0f-9fd6-ae6eadaab2ec",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "（2) 填充输入 Padding the inputs\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4c6044-e213-47c5-92f4-04706eca3084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\\ntensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\\ntensor([[ 1.5694, -1.3895],\\n        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)\n",
    "\n",
    "# 输出\n",
    "'''\n",
    "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
    "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
    "tensor([[ 1.5694, -1.3895],\n",
    "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a3ccbd-f24d-4bbd-883c-c731765514ec",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "5. 总结 Putting it all together\n",
    "    \n",
    "我们已经探索了分词器的工作原理，并研究了分词 tokenizers、转换为输入 ID conversion to input IDs、填充 padding、截断 truncation和注意力掩码 attention masks。Transformers API 可以通过高级函数为我们处理所有这些。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ada26fef-1a73-41ac-aa84-8ae78cb0e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# 可以标记单个序列\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs)\n",
    "\n",
    "# 还可以一次处理多个序列\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "model_inputs = tokenizer(sequences)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b9e8f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# 可以根据几个目标进行填充\n",
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0559e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# 还可以截断序列\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "print(model_inputs)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95eef82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n",
      "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# 可以处理到特定框架张量的转换，然后可以将其直接发送到模型。\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e8fcd-dc12-4c75-88b2-26fd9c7450e2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "Special tokens: 分词器在开头添加特殊词[CLS]，在结尾添加特殊词[SEP]。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "001bf112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\"[CLS] i\\'ve been waiting for a huggingface course my whole life. [SEP]\"\\n\"i\\'ve been waiting for a huggingface course my whole life.\"\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "# print(model_inputs)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "# 输出\n",
    "'''\n",
    "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
    "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
    "'''\n",
    "\n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))\n",
    "\n",
    "# 输出\n",
    "'''\n",
    "\"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\"\n",
    "\"i've been waiting for a huggingface course my whole life.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6386d591-e2b5-46a4-9d5b-941fdd2b828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# 总结：从分词器到模型\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "output = model(**tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e925226-6cc7-44be-b714-096755cd9ae2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: green; padding: 10px\">\n",
    "二. 基于Transformers库微调预训练模型 Fine-tuning\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d93b70-f19a-4347-b54a-a0bb018e2b43",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "一、 处理数据 Processing the data\n",
    "    \n",
    "1. 如何在 PyTorch 中训练一个batch批次的序列分类器 sequence classifier ：\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08f7fc1a-71d4-47fb-8a3d-13728b7b8818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/CX/PyProject/venv/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a707e5-451e-4a35-9328-483d963b13d7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "2. MRPC（微软研究释义语料库）数据集的例子\n",
    "\n",
    "该数据集由 5,801 对句子组成，带有一个标签，表明它们是否是翻译（paraphrases）（即，如果两个句子的意思相同）\n",
    "这是构成GLUE 基准的 10 个数据集之一，用于衡量 ML 模型在 10 个不同文本分类任务中的性能。\n",
    "\n",
    "(1) 导入数据 Loading a dataset from the Hub\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3c042bf-627a-476a-bf1f-4213bdc0152c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\n",
    "raw_datasets\n",
    "\n",
    "# 输出\n",
    "# DatasetDict包含训练集、验证集和测试集，训练集中有 3,668 对句子，验证集中有 408 对，测试集有 1,725对。\n",
    "# 每一个都包含 sentence1、sentence2、label和idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "023d3939-ee82-4973-b852-cd98f2a65ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'The Nasdaq composite index inched up 1.28 , or 0.1 percent , to 1,766.60 , following a weekly win of 3.7 percent .',\n",
       " 'sentence2': 'The technology-laced Nasdaq Composite Index .IXIC was off 24.44 points , or 1.39 percent , at 1,739.87 .',\n",
       " 'label': 0,\n",
       " 'idx': 114}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_datasets = raw_datasets[\"train\"]\n",
    "raw_train_datasets[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00044956-49c2-4aa3-9086-82cc01902108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_datasets.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc24d0a5-7814-48a1-8945-3bee9ace5da1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "(2) 预处理数据集 Preprocessing a dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80f53633-51bd-4804-8e53-aa4222b2cbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 6251, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoints = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "# 直接分词每对的所有第一个句子和所有第二个句子\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n",
    "\n",
    "# 分词器还可以采用一对序列并按照我们的 BERT 模型期望的方式进行准备\n",
    "inputs = tokenizer(\"this is the first sentence.\",\"this is the second sentence.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfefd87d-55a3-4556-8bbc-5555569db37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 告诉模型输入的哪一部分是第一句话，哪一部分是第二句话\n",
    "tokenizer.convert_ids_to_tokens(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "343d790a-a11d-406f-ac22-1854766461d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"],example[\"sentence2\"],truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14a5db28-2c43-4295-b5b4-0a2ba88dec67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da817eb574748cc97aa494fafa30346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用 map 的方法\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function,batched = True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5700df4-4306-4cc7-acea-6b16408ad155",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "(3) 动态填充 Dynamic padding\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b5bcde7-2ac5-42bd-a9d3-16de989b226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "samples = tokenized_datasets[\"train\"][:8] #取训练集中前8个样本\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]} #去除这三个特征，因为用不到\n",
    "[len(x) for x in samples[\"input_ids\"]] # 每个样本的长度 [50, 59, 47, 67, 59, 50, 62, 32]\n",
    "\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "# 输出\n",
    "'''\n",
    "{'attention_mask': torch.Size([8, 67]),\n",
    " 'input_ids': torch.Size([8, 67]),\n",
    " 'token_type_ids': torch.Size([8, 67]),\n",
    " 'labels': torch.Size([8])}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6268d-9ed9-4c8c-99bc-931ad03553e7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "二、 使用 Trainer API 微调模型 Fine-tuning a model with the Trainer API\n",
    "    \n",
    "Transformers 提供了一个Trainer类来帮助您微调它在数据集上提供的任何预训练模型。\n",
    "最困难的部分可能是准备运行环境Trainer.train()，因为它在 CPU 上运行速度非常慢。\n",
    "\n",
    "1. 训练 Training\n",
    "\n",
    "定义一个TrainingArguments类，该类将包含Trainer将用于训练和评估的所有超参数。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7c4b073-4487-4227-8f6e-da5b4c6c8ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=no,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=test-trainer/runs/Sep15_16-48-29_yangsiqideMacBook-Pro.local,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "output_dir=test-trainer,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=test-trainer,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80bdbc-86d2-4c6d-9825-f5f3b0a3ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义我们的模型\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "336438b5-7d9f-4b5d-9373-fe4aa78d2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以通过向它传递所构建的对象来定义一个Trainer，也可以通过后面（三）中的方法，自己定义训练方法\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9f70e-eb6f-4c9b-b11b-a22c52338aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d759e8-66ba-46cd-8d63-7009a618ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用train()方法进行训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97840f86-8065-4261-9cc1-7cc8d057eb00",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "2. 评估 Evaluation\n",
    "    \n",
    "使用以下Trainer.predict()命令\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf51b1-a8f1-45f9-ade6-6946f03907fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出predict()方法是另一个名为元组三个字段：predictions，label_ids，和metrics\n",
    "\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4eca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要将它们进行转换，才可以将这些preds与标签进行比较，去构建我们的compute_metric()函数\n",
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将上面的整合在一起\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd27b15-ff8a-4259-832c-fbe84a4193ef",
   "metadata": {},
   "source": [
    "<div style=\"background-color: green; padding: 10px\">\n",
    "三、 完整的训练 A full training\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d5e7a-edae-4ba3-a0ad-1061a01d90d2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "1. 载入数据\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa1e10-0f9d-42f0-aa75-76bf4428e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据 \n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abb9af-834a-4cac-8f6b-078d4b6e1f18",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "2. 数据预处理\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2043122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names  # [\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8baa6-5fd8-4f44-bdae-b2f3f8818a70",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "3. 定义我们的数据加载器\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9538dfc-a822-446f-9653-a2417374a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义我们的数据加载器\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b1c38-259a-4242-b4ed-1b11b13a6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了快速检查数据处理中没有错误，我们可以这样检查一个批次：\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "# 输出\n",
    "'''\n",
    "{'attention_mask': torch.Size([8, 65]),\n",
    " 'input_ids': torch.Size([8, 65]),\n",
    " 'labels': torch.Size([8]),\n",
    " 'token_type_ids': torch.Size([8, 65])}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca939845-cfa9-40fc-a8bb-db200a023eba",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "4. 模型\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81272554-131b-43e8-8479-d7ec16a568b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# 为了确保训练过程中一切顺利，我们将数据分批次传递给这个模型\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape) # tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553f352-9f54-43b8-8cd5-4d291fb6e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traineris使用的优化器 AdamW 与 Adam 相同，但对权重衰减正则化有所不同\n",
    "\n",
    "'''\n",
    "默认情况下，优化器会线性衰减学习率，对于上面的例子，学习率会线性地从 5e-5降到 0。（通常会产生更好的效果）\n",
    "为了正确地定义学习率调度器，我们需要知道总的训练步数 (step)，它等于训练轮数 (Epoch number) 乘以每一轮中的步数（也就是训练 dataloader 的大小）\n",
    "'''\n",
    "\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "from transformers import get_scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps) # 1377"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a7a98-6342-4610-adcb-558b97bd2759",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "5. 使用GPU\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a66ae5-2407-4e23-b109-58a831009a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果我们可以访问 GPU，我们将希望使用 GPU（在 CPU 上，训练可能需要几个小时而不是几分钟）。\n",
    "# 为此，我们定义了一个device我们将放置我们的模型和我们的批次\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device # device(type='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd80d9-3545-4135-a836-9720b24faa55",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "6. 添加进度条\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04dd0f-bcea-48a4-aa53-8fd2ffdb3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tqdm库在训练步骤数上添加了一个进度条：\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# 开始训练\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da6640-ecc8-4e90-b3e1-88c52e088aae",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "7. 评估\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d2adc-1551-4204-a855-972babb56284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute() # {'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b2ea9-b9ce-4cfa-864b-950f13eee8e6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: orange; padding: 10px\">\n",
    "8. 使用Accelerate增强您的训练循环\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3aea4-1869-4e40-b097-332d840d6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Accelerate增强您的训练循环\n",
    "# 只需进行一些调整，我们就可以在多个 GPU 或 TPU 上启用分布式训练。\n",
    "\n",
    "'''以下是未使用Accelerate模块时的训练代码'''\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c900af-9c9a-40b8-a033-8f3634091550",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''以下是使用Accelerate模块时的训练代码'''\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f839bd-f08c-435e-80a1-4a97a247ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''使用Accelerate模块前后的变化对比'''\n",
    "\n",
    "+ from accelerate import Accelerator\n",
    "  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "+ accelerator = Accelerator()\n",
    "\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "  optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "- device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "- model.to(device)\n",
    "\n",
    "+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "+     train_dataloader, eval_dataloader, model, optimizer\n",
    "+ )\n",
    "\n",
    "  num_epochs = 3\n",
    "  num_training_steps = num_epochs * len(train_dataloader)\n",
    "  lr_scheduler = get_scheduler(\n",
    "      \"linear\",\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=num_training_steps\n",
    "  )\n",
    "\n",
    "  progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(num_epochs):\n",
    "      for batch in train_dataloader:\n",
    "-         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "-         loss.backward()\n",
    "+         accelerator.backward(loss)\n",
    "\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "          progress_bar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
